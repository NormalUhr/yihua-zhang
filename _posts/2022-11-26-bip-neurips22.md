---
layout: paper
title:  "[NeurIPS22] Advancing Model Pruning via Bi-level Optimization"
date: 2022-11-26 21:00:00
author: "<a style='color: #dfebf7' href='https://www.yihua-zhang.com/'>Yihua Zhang</a><sup>[1]</sup>*,
         <a style='color: #dfebf7' href='https://www.cse.msu.edu/~yaoyugua/'>Yuguang Yao</a><sup>[1]</sup>*,
         <a style='color: #dfebf7' href='https://rithram.github.io/'>Parikshit Ram</a><sup>[2]</sup>,
         <a style='color: #dfebf7' href='https://puzhao.info/'>Zhao Pu</a><sup>[3]</sup>,
         <a style='color: #dfebf7' href='https://tianlong-chen.github.io/about/'>Tianlong Chen</a><sup>[4]</sup>,
         <a style='color: #dfebf7' href='https://people.ece.umn.edu/~mhong/mingyi.html'>Mingyi Hong</a><sup>[5]</sup>,
         <a style='color: #dfebf7' href='https://web.northeastern.edu/yanzhiwang/'>Yanzhi Wang</a><sup>[3]</sup>,
         <a style='color: #dfebf7' href='https://lsjxjtu.github.io/'>Sijia Liu</a><sup>[1,2]</sup>"
affiliation: "<sup>[1]</sup>Michigan State University, <sup>[2]</sup>IBM Research, <sup>[3]</sup>Northeastern University, <sup>[4]</sup>University of Texas at Austin, <sup>[5]</sup>University of Minnesota, Twin City"
code: "https://github.com/OPTML-Group/BiP"
poster: "https://www.yihua-zhang.com/assets/posters/bip.jpg"
paper: "https://arxiv.org/pdf/2210.04092.pdf"
tags: "ScalableML"
categories: "paper"
---

#### Dilemma in Model Pruning: Effective or Efficient?

Among the many powerful pruning methods, Iterative Magnitude Pruning (IMP) is one of the most significant and popular methods, which prunes the model in an iterative manner and can reach extremely sparse level without any performance loss. However, it often consumes much more than training a dense model from scratch. In contrast, efficient one-shot pruning methods can not deliver a high-quality sparse subnetwork, as shown in Figure 1.
Therefore, existing pruning methods have reached a dilemma over choosing between the effective method and the efficient method.


<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/posts/bip_nips22/dilemma.png" title="Dilemma in Pruning" class="img-fluid rounded z-depth-1" zoomable=true %}
    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 1. The dilemma in model pruning: Powerful pruning methods (e.g., IMP) suffer from high computational overhead.
</div>


<center>
<b>
How to advance the optimization foundation of model pruning to achieve high accuracy and pruning efficiency?
</b>
<br>
</center>

---

#### Model Pruning as a Bi-level Optimization Problem

We start our research by revisiting the model pruning problem and reformulate it to a bi-level optimization (BLO) problem:


$$
 \min_{\mathbf{m} \in \mathcal{S}} \ell(\mathbf{m} \odot \boldsymbol\theta^{\ast}(\mathbf{m})) \quad \quad \text{subject to} \boldsymbol\theta^{\ast}(\mathbf{m}) = \text{argmin}_{\boldsymbol\theta \in \mathbb{R}^n} \ell(\mathbf{m} \odot \boldsymbol\theta + \frac{\lambda}{2}\|\boldsymbol\theta\|_2^2),
$$

where the upper-level problem optimizes the pruning mask for the model and the lower-level retrains the model with the fixed mask. The benifits from this bi-level formulation are two-folded.

* First, we have the flexibility to use the mismatched pruning and retraining objectives.

* Second, the bi-level optimization enables us to explicitly optimize the coupling between the retrained model weights and the pruning mask through the implicit gradient (IG)-based optimization routine.


#### Optimization Foundation of BIP

BLO is different from other optimization problems, as the gradient descent of the upper-level variable will involve the calculation of the implicit gradient (IG).

$$
\frac{d\ell(\mathbf{m}\odot\boldsymbol\theta^\ast(\mathbf{m}))}{d\mathbf{m} = \nabla_{\mathbf{m}}\ell(\mathbf{m}\odot\boldsymbol\theta^\ast(\mathbf{m}))} + \frac{{\boldsymbol\theta^\ast(\mathbf{m})}^\top}{d\mathbf{m}} \nabla_{\boldsymbol\theta}\ell(\mathbf{m}\odot\boldsymbol\theta^\ast(\mathbf{m}))
$$

The IG challenge is a fingerprint of the BLO solver and derives from the implicit function theory. It refers to the gradient of the lower-level solution w.r.t. the upper-level variable and in most cases is very difficult to calculate. The main reason is it usually involves the second order derivative and matrix inversion.

$$
\frac{{\boldsymbol\theta^\ast(\mathbf{m})}^\top}{d\mathbf{m}} = -\nabla^2_{\mathbf{m}\boldsymbol\theta}\ell(\mathbf{m}\odot\boldsymbol\theta^\ast)[\nabla^2_{\boldsymbol\theta}\ell(\mathbf{m}\odot\boldsymbol\theta^\ast) + \lambda \mathbf{I}]^{-1}
$$

Very luckily, in the model pruning scenario, the upper- and lower-level variables are always combined as a bi-linear variable. We can thus leverage this bi-linear properties and derive a closed-form IG solution, which only requires the first-order derivative.

$$
\frac{{\boldsymbol\theta^\ast(\mathbf{m})}^\top}{d\mathbf{m}} = -\frac{}{\gamma}
$$

---

#### BiP: Bi-level Optimization-based Pruning


<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/posts/bip_nips22/algorithm.png" title="BiP algorithm overview" class="img-fluid rounded z-depth-1" zoomable=true%}
    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 2. The pseudo-code for Bi-level Pruning (BiP) Algorithm.
</div>


<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/posts/bip_nips22/bip_overview.png" title="BiP algorithm visualization" class="img-fluid rounded z-depth-1" zoomable=true%}
    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 2. An illustration of the proposed BiP algorithm.
</div>


##### Comparison to IMP and OMP

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/posts/bip_nips22/imp.png" title="BiP algorithm overview" class="img-fluid rounded z-depth-1" zoomable=true%}
    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 3. Pruning pipeline visualization of IMP.
</div>

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/posts/bip_nips22/omp.png" title="BiP algorithm overview" class="img-fluid rounded z-depth-1" zoomable=true%}
    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 4. Pruning pipeline visualization of OMP.
</div>

---

#### Experiment results

##### BIP identifies high-accuracy subnetworks in unstructured pruning

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/posts/bip_nips22/exp_unstructured_pruning.png" title="Unstructured pruning result." class="img-fluid rounded z-depth-1" zoomable=true%}
    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 3. Unstructured pruning trajectory given by test accuracy (%) vs. sparsity (%) on various (dataset, model) pairs. The performance of the dense model and the best winning ticket are marked using dashed lines in each plot. The solid line and shaded area of each pruning method represent the mean and variance of test accuracies over 3 trials.
</div>

##### BIP identifies high-accuracy subnetworks in structured pruning

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/posts/bip_nips22/exp_structured_pruning.png" title="Structured pruning result." class="img-fluid rounded z-depth-1" zoomable=true%}
    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 4. Filter-wise pruning trajectory given by test accuracy (%) vs. sparsity (%). Other settings strictly follow Figure 3.
</div>

##### BiP achieves high pruning efficiency.

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/posts/bip_nips22/exp_time.png" title="Structured pruning result." class="img-fluid rounded z-depth-1" zoomable=true%}
    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 5. Time consumption comparison on (CIFAR-10, ResNet-18) with different pruning ratio p.
</div>


##### BiP requires no rewinding.

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/posts/bip_nips22/no_rewinding.png" title="BiP requires no rewinding." class="img-fluid rounded z-depth-1" zoomable=true%}
    </div>
</div>
<div class="caption" style="color: #999; font-size:16px; padding: 2px;">
    Figure 6. The sensitivity of BIP to rewinding epoch numbers on different datasets and model architectures. "N/A" in the x-axis indicates BIP without retraining.
</div>

---

#### Citation

```
@inproceedings{zhang2022advancing,
  title = {Advancing Model Pruning via Bi-level Optimization},
  author = {Zhang, Yihua and Yao, Yuguang and Ram, Parikshit and Zhao, Pu and Chen, Tianlong and Hong, Mingyi and Wang, Yanzhi and Liu, Sijia},
  booktitle = {Advances in Neural Information Processing Systems},
  year = {2022}
}
```
---

#### Reference

<div id="refer-anchor-1"></div> [1] Jonathan Frankle et al. “The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks.” ICLR 2019. 

<div id="refer-anchor-2"></div> [2] Jonathan Frankle et al. “Linear Mode Connectivity and the Lottery Ticket Hypothesis.” ICML 2020.

<div id="refer-anchor-3"></div> [3] Ren Wang et al. “Practical detection of trojan neural networks: Data-limited and data-free cases.” ECCV 2020.