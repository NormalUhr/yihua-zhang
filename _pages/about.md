---
layout: about
title: About
permalink: /
subtitle: 

profile:
  align: right
  image: vanishing_me.jpg
  image_circular: false # crops the image to make it circular
  address: >
    <p>Room 3210 </p>
    <p>428 S Shaw LN</p>
    <p>East Lansing, Michigan</p>
    <p>United States of America</p>
    <p>             </p>
    <p>             </p>

news: true  # includes a list of news items
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---


Yihua Zhang (张逸骅) is a fourth-year Ph.D. student at [OPTML Group](https://www.optml-group.com/) at Michigan State University, under the supervision of [Prof. Sijia Liu](https://lsjxjtu.github.io/). His research centers on *trustworthy* and *scalable* machine learning (ML) algorithms for large language models (LLMs), multi-modal language models (MLLM), and diffusion models (DMs), with a keen focus on bridging theoretical foundations and real-world applications. In recognition of his outstanding contributions, Yihua was honored with the [IBM PhD Fellowship 2024](https://research.ibm.com/university/awards/fellowships-awardees.html), the [CPAL 2025 Risiting Star Award](https://2025.cpal.cc/rising_stars_presentations/) hosted by at Stanford Data Science, and the prestigious [MLCommons Rising Star Award in 2024](https://mlcommons.org/2024/06/2024-mlc-rising-stars/) hosted by NVIDIA. Yihua has gained valuable industry experience through internships at leading technology companies such as [Bytedance Seed](https://seed.bytedance.com/en/), [Meta AI](https://ai.meta.com/), [Amazon AWS AI Lab](https://aws.amazon.com/ai/), and [Cisco Research](https://research.cisco.com/). Yihua’s work is driven by the need to develop efficient, scalable, and robust ML algorithms, with a commitment to addressing modern challenges in these domains.

<!-- **Research Keywords**: Machine Unlearning, Jailbreak Attack, Adversarial Training, Fairness, Parameter-Efficient Fine-Tuning, Memory-Efficient Fine-Tuning, Mixture-of-Experts, Model Sparsity, Large Language Model, Diffusion Model, Bi-Level Optimization, Zeroth-Order Optimization. -->

:heavy_check_mark: **Industry: Multimodal Modeling and Large-Scale Pretraining for LLMs and VLMs**  
Yihua’s industrial research experience spans both [Meta AI](https://ai.meta.com/) and [Bytedance Seed](https://seed.bytedance.com/en/), where he contributed to developing the next generation of large-scale multimodal foundation models. 
* At Meta, he built and deployed SOTA fusion and alignment algorithms across 8–10 distinct modalities, which have been integrated into Meta’s internal production systems, achieving remarkable progress in multimodal ads ranking and unified modeling. His work led to the design and scalable training of industry-level multimodal foundation models that combine vision, text, audio, tabular, time-series, and structured data within a unified framework  
* At Bytedance Seed, he focused on token-efficient multimodal fusion for Seed-VLM pretraining, designing novel optimizers to improve convergence, stability, and token utilization efficiency. 
* Yihua gained extensive large-scale distributed training experience, working on multi-node (32–128 nodes, >256–1024 A100/H100) systems, leveraging advanced frameworks, such as Megatron and FSDP2, with a deep understanding of parallel strategies including TP (tensor), PP (pipeline), SP (sequence), and EP (expert) parallelism. These experiences have strengthened his ability to bridge algorithmic innovation with production-grade deployment of multimodal foundation models at scale.


**I am on the academic and industry job market right now! If you know any role that might be a good fit, please feel free to contact me!**


:heavy_check_mark: **Theme 1: Trustworthy Foundation Models: Robustness, Fairness, and Unlearning**: Yihua explores how to enhance the trustworthiness of foundation models, focusing on robustness against adversarial attacks, fairness in decision-making, and the emerging area of machine unlearning to ensure data privacy and compliance with deletion requests.

:heavy_check_mark: **Theme 2: Scalable Foundation Models: Efficient Models, Data, and Algorithms**: In this theme, Yihua's work revolves around designing models that are not only powerful but also computationally efficient. His research includes advancements in model sparsification, memory-efficient fine-tuning techniques, and optimizing data usage for large-scale models.

:heavy_check_mark: **Theme 3: Optimization in Modern ML: Bi-Level and Zeroth-Order Optimization**
This research line focuses on the theoretical underpinnings of scalable machine learning algorithms, addressing real-world constraints through bi-level optimization and zeroth-order optimization.


**Collaboration Opportunities**

I am always open to collaborations with researchers, as well as undergraduate and graduate students seeking Ph.D. positions. While my primary research focuses on trustworthy and scalable ML algorithms for LLMs and DMs, I am also interested in exploring a wide range of topics beyond these areas. If you have exciting research ideas or are looking for opportunities to conduct research under professional guidance, feel free to reach out to me. Please refer to my [collaboration statement](./collaboration) for more details. You are also welcome to befriend me on [Wechat](./assets/img/Wechat.jpg) or connect me through [LinkedIn](https://www.linkedin.com/in/zhangyihua/).

